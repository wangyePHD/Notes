
## Multifold Cross-modal Masked Pre-training 初期实验方案设计

### 实验一 Vanilla PiMAE Pre-training
由于torch版本的不同，同时原始repo也没有给出具体的torch版本信息，当使用和PiMAE相同的batchsize (128)设置时，会报错，这个error是由于torch版本不同带来的bug。当降低batchsize至100时，可以正常预训练。

#### 疑问
1. 为什么经过SUN RGB-D数据集的预训练之后，在SUN RGB-D数据集上微调3DETR还需要1080轮呢？毕竟3DETR的training from the scratch才是1080,那这样的话，预训练的意义是什么呢？
    * 在PointMAE的issue中，原作者进行了回复。简单来说，就是看效果，finetune的epoch如果较少的话，那效果就不行。

### 20230411 实验 PointMAE vs 2-fold PointMAE

**Baseline**:
采用原始的PointMAE模型，发表于ECCV2022。属于纯点云模态的mask pre-training模型。并不是当前点云自监督预训练领域的SOTA.
![111](../../../00_Assets/2023-04-11-10-21-01.png)
**2-fold PointMAE**:
每一个点云random mask两次，其余settings均和baseline保持一致。

**预训练数据集**：
1. ShapeNet-55

**下游任务**：
1. ModelNet40 finetuning task
2. ModelNet40 10-fold few-shot learning task
    * 5-way, 10-shot 
    * 5-way, 20-shot
    * 10-way, 10-shot
    * 10-way, 20-shot

**实验结果**
1. Finetuning on ModelNet40 dataset.

|  Methods | Accuracy  |
|---|---|
| Point-MAE | **93.8%**  |
| 2-fold Point-MAE | 93.3% |

发现：
* 2-fold Point-MAE在modelnet上进行finetuning，最终的效果有所下降。目前分析出来的原因是可能是2-fold PointMAE的预训练时间过长，从而导致过拟合。缩短预训练时长的实验目前正在刷。



2. Few-shot learning on ModelNet40 dataset.

|  Methods | 5-ways,10-shot  | 5-ways,20-shot | 10-ways,10-shot | 10-ways,20-shot  |
|---|---|---|---|---|
|  Point-MAE | 96.3 $\pm$ 2.5 | 97.8 $\pm$ 1.8 | 92.6 $\pm$ 4.1 | 95.0 $\pm$ **3.0**  |
|  2-fold Point-MAE | **96.6** $\pm$ 2.62 | **98.0** $\pm$ **1.0** | 92.8 $\pm$ 3.96 | 95.2 $\pm$ 3.11  |
|  3-fold Point-MAE | 96.5 $\pm$ **2.06** | **98.0** $\pm$ 1.34 | **92.85** $\pm$ **3.74** | **95.35** $\pm$ 3.17  |

发现：
* 2-fold PointMAE在few-shot learning上均带来了提升。可能是因为预训练阶段，对于每一个shape都学习了更多的信息，最终学出来的representation在小样本设置下性能更优。


**现阶段问题**
1. multifold mask setting 相对于vanilla PointMAE增大了batchsize，同时预训练阶段+微调大概需要30个小时，太慢了。
    * 目前的解决办法就是设计一个在小数据集上预训练的实验方案
    * 迁移至4*A10服务器进行实验（运行中）
