## 3D_MAE
1. Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining, CVPR 2023, SOTA
2. GD-MAE: Generative Decoder for MAE Pre-training on LiDAR Point Clouds, 2023.03.17, CVPR 2023 （掩码策略）
3. Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?, ICLR 2023
4. Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling, CVPR 2022
5. PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection, 2023.03.14 CVPR2023
6. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training, 2022.05.28
7. Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders, 2022.12
8. POS-BERT: Point Cloud One-Stage BERT Pre-Training, 2022.04.03
9. Masked Autoencoders for Self-Supervised Learning on Automotive Point Clouds, 2022.07
10. Voxel-MAE: Masked Autoencoders for Pre-training Large-scale Point Clouds, 2022.06.20
11. Masked Autoencoders for Point Cloud Self-supervised Learning, 2022.03.13
12. Masked Autoencoders in 3D Point Cloud Representation Learning，2022.07.04


## 3D 对比学习
1. PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding, 2022.06
2. P4Contrast: Contrastive Learning with Pairs of Point-Pixel Pairs for RGB-D Scene Understanding. 2020.12.13
3. Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts, 2020.12
4. Self-Supervised Pretraining of 3D Features on any Point-Cloud, 2021.01.07
5. SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-Training for Spatial-Aware Visual Representations, 2021.12.09


## 其他
1. AI Choreographer: Music Conditioned 3D Dance Generation with AIST++, 2021.01.21
1. MaskGIT: Masked Generative Image Transformer
2. Mixed Autoencoder for Self-supervised Visual Representation Learning,2023.03.20
3. MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning,2022.05.26
4. Are Large-scale Datasets Necessary for Self-Supervised Pre-training?, 2021.12.20
5. MAE: Masked Autoencoders Are Scalable Vision Learners, 2021.12.09
6. SimMIM: a Simple Framework for Masked Image Modeling, 2021.11.18
7. Multimodal Masked Autoencoders Learn Transferable Representations, ICLR 2023 reject, 2022.10.21, https://github.com/young-geng/m3ae_public
8. Connecting representation and generation via masked vision-language transformer, ICLR 2023 reject, 2023.02.02
9. MultiMAE: Multi-modal Multi-task Masked Autoencoders, 2022.04.04, ECCV2022
10. Scaling Language-Image Pre-training via Masking, 2022.12.01, Kaiming He
11. VLMAE: Vision-Language Masked Autoencoder, 2022.08.19
12. Language-Driven Representation Learning for Robotics, 2023.02.24
13. Masked Contrastive Pre-Training for Efficient Video-Text Retrieval, 2022.12.05
14. MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning, 2022.12.09
15. GLIPv2: Unifying Localization and VL Understanding, 2022.10.11
16. ConvMAE: Masked Convolution Meets Masked Autoencoders
17. i-MAE: Are Latent Representations in Masked Autoencoders Linearly Separable?

